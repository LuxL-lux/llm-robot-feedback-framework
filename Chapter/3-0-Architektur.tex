Das entwickelte Framework wird innerhalb der Laufzeit- und
Entwicklungsumgebung der Software Unity3D des
Unternehmens Unity Technologies implementiert. Es basiert konzeptuell auf
der simulierten Ausführung von Roboterprogrammcode mittels eines virtuellen
Controllers innerhalb der Software RobotStudio des Unternehmens
ABB und der Übertragung relevanter Bewegungs- und Zustandsdaten in
Unity3D. Dort wird ein baugleicher Roboter mittels eines
3D-CAD-Modells abgebildet, welcher so den in RobotStudio simulierten
Roboter in annähender Echtzeit abbildet. Darauf aufbauend werden die
Überwachungsmodule (Monitore) integriert, welche unterschiedliche
Aspekte des Roboterverhaltens beobachten und Abweichungen vom gewünschten
Verhalten erkennen und dokumentieren.

Im Folgenden wird erläutert, wie das Framework funktional, architektonisch und
algorithmisch aufgebaut und implementiert ist. Grundlegend werden
dabei für das Framework folgende Festlegungen vorgenommen, welche
sich in Abschnitt~\ref{sec:testumgebung} wiederfinden: Als
Testobjekt wird ein ABB IRB 6700-235/2.65 Roboter verwendet. Dabei handelt es
sich um einen Knickarmroboter mit 6 Freiheitsgraden und 6
Gelenken.\vglcite{abbirb2025} Dieser wird in RobotStudio Version 2025.3
simuliert und mittels eines OmniCore-Controllers gesteuert.

\subsection{Funktionale Anforderungen}

Die funktionalen Anforderungen an das Framework
lassen sich in zwei Hauptkategorien unterteilen: die Echtzeitüberwachung
kritischer Roboterzustände und Programmablaufereignisse sowie die
systematische Protokollierung der auftretenden Ereignisse.

\subsubsection{Überwachung kritischer Roboterzustände}

Das Framework soll vier zentrale Überwachungsfunktionen implementieren:

\begin{enumerate}
  \item \textbf{Kollisionserkennung}: Identifikation von Kontakten
    zwischen Roboterkomponenten und Objekten in der Arbeitsumgebung
    sowie Selbstkollisionen zwischen benachbarten Gliedern der
    kinematischen Kette.

  \item \textbf{Singularitätserkennung}: Frühzeitige Erkennung
    kinematischer Singularitäten (Handgelenk-, Schulter- und
    Ellbogensingularitäten), die zu Kontrollverlust oder
    undefinierten Bewegungen führen können.

  \item \textbf{Prozessflussüberwachung}: Validierung der korrekten
    Abfolge von Bearbeitungsstationen für Werkstücke und Erkennung
    von Abweichungen von der spezifizierten Sequenz.

  \item \textbf{Gelenkdynamiküberwachung}: Kontinuierliche
    Überprüfung von Gelenkwinkeln, -geschwindigkeiten und
    -beschleunigungen auf Grenzwertüberschreitungen, insbesondere
    während der Handhabung von Werkstücken.
\end{enumerate}

\subsubsection{Systematische Ereignisprotokollierung}

Mithilfe des Frameworks soll es möglich sein, die genannten Fehlerarten
systematisch debuggen zu können und die Fehlersuche in Roboterprogrammcode zu
vereinfachen. Neben der reinen Erkennung ist zur späteren Analyse und
Fehlersuche die formalisierte
Dokumentation der Fehlerereignisse notwendig. Bei der Dokumentation
soll jedes detektierte Ereignis um
entsprechende Metadaten zum aktuellen Status sowie hilfreiche Daten zur
weiteren Fehleranalyse des Roboters ergänzt werden.
Folgende Metadaten lassen sich nach initialem Testen mithilfe der zur Verfügung
stehenden Tools extrahieren:

\begin{itemize}
  \item \textbf{Zeitlicher Kontext}: Präziser Zeitstempel des
    Ereignisses mit Millisekunden-Auflösung
  \item \textbf{Räumlicher Kontext}: Vollständige
    Gelenkwinkelkonfiguration und TCP-Position zum Ereigniszeitpunkt
  \item \textbf{Programmkontext}: Aktuelles Modul, Routine und
    Programmzeile der Robotersteuerung
  \item \textbf{Ereignisspezifische Daten}: Je nach Ereignistyp
    relevante Zusatzinformationen (Kollisionspunkt, Singularitätstyp,
    Sequenzabweichung, Grenzwertüberschreitung)
\end{itemize}

Diese strukturierte Erfassung ermöglicht die nachgelagerte genaue Auswertung der
vorhandenen Daten und soll das Fehlersuchen (Debuggen) im Roboterprogrammcode
erleichtern.

\subsection{Zielsetzung und architektonische Anforderungen}
Der Stand der Technik aktueller Simulationsplattformen für Roboter zeigt, dass
virtuelle Steuerungen und damit zusammenhängende Simulationsplattformen
herstellerspezifisch entwickelt und mit Ausnahme von ROS keine einheitliche
Schnittstelle zur Kommunikation mit externen Plattformen oder Physik-Engines
bieten. Somit muss für jeden Robotertyp ein designierter Konnektor genutzt
werden, um diesen mit einer externen, herstellerunabhängigen
Simulationsplattform
zu verbinden. Um eine herstellerunabhängige und erweiterbare Plattform zu
entwickeln, sollte also eine gemeinsame Schnittstelle implementiert werden.

Um dies zu ermöglichen, verfolgt die Architektur des Frameworks drei
zentrale Ziele:
\begin{enumerate}
  \item \textbf{Vendor-Agnostik}: Abstraktion verschiedener
    Roboterhersteller durch einheitliche Interface-basierte
    Architektur ohne herstellerspezifische Abhängigkeiten im Kern-Framework

  \item \textbf{Modulare Erweiterbarkeit}: Plugin-System für Safety
    Monitoring Module und Kommunikationsprotokolle ohne Änderungen
    der bestehenden Architektur

  \item \textbf{Echtzeitfähige Kommunikation}: Latenzarme
    Datenübertragung für Motion Control und ereignisbasierte
    Sicherheitsüberwachung
\end{enumerate}

\subsection{Unity3D als Simulationsplattform}

Unity3D wird in dieser Arbeit ausschließlich als
Simulations\-laufzeit verwendet.
Die Engine stellt Szene, Physik und Rendering bereit und bietet aufgrund ihrer
3D/Physik-Infrastruktur eine plattformübergreifende Grundlage für den
digitalen Abschnitt des Roboters.\vglcite[247]{andaluz2016} Die eigentliche
Anwendungslogik wird durch klar definierte Adapter von der Engine entkoppelt,
wodurch eine strikte Trennung zwischen Simulation und Domäne gewahrt bleibt.
Asynchrone Abläufe im Update-Loop dienen lediglich der Taktung und greifen
nicht direkt auf Unity-APIs zu. Die Wahl von Unity begründet sich zudem durch
die breite Tooling-Unterstützung und die Möglichkeit, sowohl visuelle als auch
programmatische Arbeitsabläufe zu kombinieren.\vglcite[431]{bartneck2015}

\subsection{Systemarchitektur}

Das entwickelte Framework implementiert eine vierschichtige Architektur, die
eine klare Trennung der Verantwortlichkeiten gewährleistet (vgl. Abbildung
\ref{fig:layer_architecture}). Diese Strukturierung folgt etablierten
Software-Engineering-Prinzipien, um Wartbarkeit und Erweiterbarkeit zu
gewährleisten.

\begin{figure}[H]
  \centering
  \includegraphics[width=10cm]{figures/LayerArchitekturFramework.png}
  \caption{Schichtenarchitektur des Frameworks. Orange umrandete
    Module implementieren formalisierte Interfaces
    (\texttt{IRobotSafetyMonitor}, \texttt{IRobotConnector}). Das
    \texttt{RobotState}-Objekt dient als gemeinsame Datenstruktur
  zwischen den Schichten.}
  \label{fig:layer_architecture}
\end{figure}

Die Architektur gliedert sich in vier logisch getrennte Schichten:

\begin{enumerate}
  \item \textbf{Unity3D Framework}: Bereitstellung der
    Simulationsumgebung und Physik-Engine
  \item \textbf{Monitoring Layer}: Implementierung der Sicherheitsmonitore
  \item \textbf{Core Layer}: Zentrales State-Management und Event-Koordination
  \item \textbf{Robot Communication Layer}: Adapter für
    herstellerspezifische Robotersteuerungen
\end{enumerate}

% Ergänzende Beschreibung der drei oberen Schichten. Die Layer werden
% jeweils kurz nach Zweck, Funktion und Schnittstellen eingeordnet,
% um eine einheitliche Detailtiefe herzustellen.

\paragraph{Unity3D Framework (Simulationsschicht)}
\begin{itemize}
  \item \textbf{Zweck}: Bereitstellung der Simulationslaufzeit.
  \item \textbf{Funktion}: Taktung des Systems (Update-Loop),
    Szenenhosting und Bootstrap der Adapter. Abseits des
    Szenenlayouts und der Darstellung der Objekte soll hier keine
    tiefergehende Prozesslogik implementiert werden.
  \item \textbf{Schnittstellen}: Registrierung der Adapter über ein
    Dependency-Injection-Konzept; Monitore greifen nicht direkt auf
    Engine-APIs, lediglich Collider-Objekte werden weitergegeben.
\end{itemize}

\paragraph{Core Layer}
\begin{itemize}
  \item \textbf{Zweck}: Domänenkern für den Roboterzustand und die
    Ereignisverteilung.
  \item \textbf{Funktion}: Aktualisierung des
    \texttt{RobotState}, Verteilung von Zustandsänderungen an
    registrierte Monitore (Observer) und Aggregation von \texttt{SafetyEvent}s.
  \item \textbf{Schnittstellen}: \texttt{RobotManager},
    \texttt{RobotSafetyManager} und \texttt{RobotState} bilden die
    Kernklassen für Dispatch und Aggregation \vglcite[293\psq]{Gamma1994}.
\end{itemize}

\paragraph{Monitoring Layer}
\begin{itemize}
  \item \textbf{Zweck}: Kapselung der Prüfregeln als Policies für
    einzelne Fehlertypen (Kollision, Prozessfolge, Singularität, Dynamik).
  \item \textbf{Funktion}: Erzeugung normalisierter
    \texttt{SafetyEvent}s und Meldung von Grenzverletzungen.
  \item \textbf{Schnittstellen}: Einheitliches Kerninterface
    \texttt{IRobotSafetyMonitor} definiert die Bindung für alle Monitore.
\end{itemize}

Der \texttt{RobotManager} im Core Layer fungiert als zentraler
Event-Dispatcher, der Zustandsänderungen an alle registrierten
Monitoring-Komponenten propagiert. Diese Implementierung des Observer Patterns
ermöglicht eine lose Kopplung zwischen den
Komponenten\vglcite[293-303]{Gamma1994}: Die Sicherheitsmonitore müssen
lediglich das \texttt{IRobotSafetyMonitor}-Interface implementieren und können
zur Laufzeit dynamisch registriert oder entfernt werden, ohne andere Systemteile
zu beeinflussen.

Die in Abbildung \ref{fig:layer_architecture}
orange markierten Module definieren standardisierte Interfaces, die als Verträge
zwischen den Schichten dienen. Diese Interface-basierte Abstraktion realisiert
das Open-Closed-Prinzip, welches festlegt, dass Komponenten einerseits offen
für Erweiterung sein sollen, andererseits geschlossen für
Veränderung\vglcite{Martin2003}: Neue Robotertypen können durch Implementierung
des \texttt{IRobotConnector}-Interfaces integriert werden, ohne den Core Layer
zu modifizieren. Analog können zusätzliche Sicherheitsmonitore über das
\texttt{IRobotSafetyMonitor}-Interface hinzugefügt werden. Diese
Architekturentscheidung gewährleistet, dass das System für Erweiterungen offen
bleibt, während die Kernfunktionalität stabil und unverändert bleibt.

Die Kommunikation zwischen den Schichten erfolgt ereignisgesteuert über das
\texttt{RobotState}-Objekt als gemeinsame Datenstruktur. Die Communication Layer
aktualisiert kontinuierlich den Roboterzustand durch WebSocket-Events für
Zustandsdaten der Robotersteuerung und HTTP-Polling für
Bewegungsdaten. Der \texttt{RobotManager}
verteilt diese Zustandsänderungen asynchron an alle registrierten Observer.
Diese Event-Driven Architecture adressiert die Anforderungen an ein
erweiterbares und gleichzeitiges Messaging-Modell. So lassen sich Events
an mehrere Teilnehmer gleichzeitig weitergeben und verarbeiten, ohne dabei auf
den Processing-Zyklus zu warten. Weiterführend werden Events getrennt
voneinander verarbeitet, das heißt der Main-Thread muss nicht auf die
Fertigstellung der Abarbeitung von Events durch Empfänger
warten.\vglcite[1\psqq]{hohpe2006}

Der \texttt{SafetyManager} aggregiert die von den Monitoren generierten
Sicherheitsereignisse und implementiert eine kontextabhängige Logging-Strategie.
Bei laufendem Roboterprogramm werden detaillierte Logs im Format
JavaScript Object Notation {JSON-Logs} zur späteren Analyse
erstellt, während im Leerlauf-Zustand nur kritische Ereignisse in die Konsole
geloggt werden. Diese Differenzierung optimiert sowohl die Performance als auch
die Nachvollziehbarkeit von Sicherheitsvorfällen.

Die gewählte Schichtenarchitektur ermöglicht die unabhängige Entwicklung und
Testung einzelner Komponenten. Horizontale Erweiterungen (neue Monitore) und
vertikale Erweiterungen (neue Robotertypen) können ohne ungewollte
Seiteneffekte auf
bestehende Komponenten implementiert werden.

\subsection{Robot Communcation Layer}
Das Robot Communication Layer organisiert die Kommunikation mit der
zugrundeliegenden Robotersteuerung und
implementiert das Interface des Typs IRobotConnector. Als Verbindungsclient
zwischen Unity3D und externer Schnittstelle des Robotersystems implementiert
dieser das IRobotConnector Interface, welches als Vorlage für eine Anbindung an
eine Robotersteuerung jeden Typs fungiert und standardisierte Methoden
implementiert (vgl. Abbildung~\ref{figure:irobotconnector}).

\begin{figure}[H]
  \inputminted[fontsize=\footnotesize]{csharp}{code-snippets/IRobotConnector.cs}
  \caption{Implementierung der IRobotConnector-Interface als Verbidung zwischen
  RobotState und Roboter-Controller}
  \label{figure:irobotconnector}
\end{figure}

Ein Interface lässt sich anhand dieses Beispiels in 3
Funktionsbereiche aufteilen: Events, Attribute und Methoden.

Das Interface definiert Events (Aktionen) die bei
definierten Zustandsänderungen
in der Laufzeit ausgeführt werden. Andere Bestandteile des Frameworks
sind in der
Lage, ein Event zu abonnieren und eine Methode zu registrieren,
welche ausgeführt
werden soll, wenn dieses Event auftritt. Ist dies der Fall, wird das
entsprechende Modul über die Änderung benachrichtigt und bekommt gegebenenfalls
neue Daten zur Verfügung gestellt. Diese eventgetriebene Kommunikation sorgt
dafür, dass einerseits alle Komponenten proaktiv auf den neusten
Stand der Daten gebracht
und gehalten werden, andererseits aber keine ressourcenblockierenden Prozesse
ausgeführt werden müssen, um gegebenenfalls Statusänderungen abzufragen.

Weiterführend definiert das Interface IRobotConnector
Attribute, welche den
aktuellen Verbindungsstatus (verbunden = \texttt{true}, getrennt =
\texttt{false}) speichern sowie das
State-Objekt des Roboters. Definiert durch die schreibgeschützten, automatisch
implementierte Eigenschaft mit einem Getter können Attribute
auch von außerhalb des RobotConnectors abgefragt werden, jedoch nicht
überschrieben.

Zuletzt gibt das Interface die Methoden \texttt{Connect} und
\texttt{Disconnect}
vor, welche hier die wichtigsten Methoden zum Verbinden und
Trennen von der jeweiligen Robotersteuerung darstellen.

Als initiale Komponente wird eine Verbindung zum
Controller eines Roboters benötigt, um den Roboter in Unity emulieren zu können
und Daten zu verarbeiten. Dazu wird hier mittels eines HTTP-Clients zur
Schnittstelle des RobotStudioSDKs über die API RobotWebServices (RWS) eine
Verbindung aufgebaut. Die Verbindung zu RobotWebServices entsteht durch einen
HTTP-Client und der Authentifizierung mit in RobotStudio festgelegten
Zugangsdaten. Anschließend kann über einen erhaltenen Cookie die Verbindung
aufrechterhalten werden und aktuelle Daten über den Roboter sowohl abgefragt,
also auch der Roboter selbst gesteuert werden.\vglcite{robotwebservices2025}

Die RWS-Schnittstelle bietet einerseits die Möglichkeiten,
aktuelle Achswinkel und TCP-Werte abzurufen, als auch
weitere Zustandsdaten der Steuerung, wie das aktuell laufende
Programm, den aktuellen Motorstatus
oder auch die Codezeile, welche aktuell vom Programmzeiger ausgeführt wird.
Weiterführend bietet RWS die Möglichkeit, aktuelle analoge und digitale
Signale abzurufen, was nötig ist, um den Greifer steuern zu können.

Bei den oben genannten Daten mit Ausnahme der Achswinkel handelt es sich um
Parameter, welche sich im Laufe eines Programmablaufs vergleichsweise selten
verändern. Daher wird hier auf den Websockets-Endpunkt von RWS zugegriffen, um
innerhalb einer Duplex-Kommunikation sich verändernde Signale oder auch
Programmstati zu empfangen. Dazu wird mithilfe des HTTP-Clients eine Anfrage zur
Subscription auf verschiedene Parameter (bspw. den ProgrammPointer) gestellt,
und anschließend eine Websockets-Session aufgebaut. Die Achswinkel werden
zeitgleich über eine asynchron endlosen Task in einer in Unity definierbaren
Frequenz abgefragt.

Der Client implementiert dabei das Interface IRobotConnector und gibt ein
RobotState-Objekt mit den empfangenen Daten an den RobotManager
weiter, welcher hier
als zentraler Koordinator des aktuellen RobotState fungiert.

\subsection{Event-Driven Architecture}
Das System nutzt ein durchgängiges Event-System für lose Kopplung:
\begin{itemize}
  \item \texttt{OnRobotStateUpdated}: Zustandsänderungen
  \item \texttt{OnConnectionStateChanged}: Verbindungsstatus
  \item \texttt{OnSafetyEventDetected}: Sicherheitsereignisse
  \item \texttt{OnMotorStateChanged}: Motorstatusänderungen
\end{itemize}

\subsection{SafetyEvent und RobotStateSnapshot}
Als für die Auswertung der Simulationsergebnisse relevantes Teil des Frameworks
wird die SafetyEvent-Klasse implementiert. Jedes Mal, wenn ein Ereignis,
welches mit der tatsächlichen Simluation eines Roboterprogramms zusammenhängt,
auftritt, wird ein SafetyEvent instanziert. Dieses wird initial von der
jeweiligen überwachenden Komponente (einem SafetyMonitor) instanziert und mit
dem aktuellsten RobotState als unveränderliches Objekt
(RobotStateSnapshot) befüllt.
Weiterführend erhält es vom jeweiligen SafetyMonitor variierende
Kontextinformationen, die später für die Auswertung des Ereignisses verwendet
werden. Zusammenfassend bestehn ein SafetyEvent aus folgenden Komponenten:
\begin{itemize}
  \item \textbf{SafetyEvent}: Unveränderliches Value Object für
    Sicherheitsereignisse
  \item \textbf{RobotStateSnapshot}: Immutable Zustandserfassung zum
    Ereigniszeitpunkt
  \item \textbf{Ereignistypen}: Info, Warning, Critical mit
    konfigurierbaren Schwellwerten
  \item \textbf{Kontextdaten}: Vollständige Roboterzustandserfassung
    für Forensik
\end{itemize}

Basierend auf den dargestellten Schichten erfolgt in den
nachfolgenden Abschnitten die Implementierung der vier
Monitore auf Basis der zuvor skizzierten Schichten.
