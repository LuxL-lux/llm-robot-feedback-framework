\chapter{Stand der Technik} \label{cap:Grundlagen}

\section{Roboterprogrammierung}
Die Roboterprogrammierung kann als die
Programmierung von industriellen Manipulatoren verstanden werden, die sich durch
ihre programmierbaren und anpassungsfähigen Eigenschaften von anderen Maschinen
abheben. Roboterprogramme enthalten präzise Anweisungen und Spezifikationen für
die Bewegung des Roboters. Der entscheidende
Vorteil der Roboterprogrammierung ist somit Flexibilität: Roboter
können durch einfache Software-Umprogrammierung für völlig unterschiedliche
Aufgaben eingesetzt werden, während herkömmliche Steuerungstechnik meist auf
vordefinierte starre Abläufe beschränkt ist.\vglcite[1\psqq]{nilsson1996}

\subsection{Verfahren der Roboterprogrammierung}
Grundsätzlich wird bei der Roboterprogrammierung zwischen manuellen und
automatischen Verfahren unterschieden.\vglcite[1]{biggs2003}
Manuelle Systeme erfordern die explizite Erstellung des Programms
durch den Anwender, wobei textbasierte Programmiersprachen (z.\,B.
  herstellerspezifische Sprachen wie KUKA Robot Language (KRL) oder ABB
RAPID) sowie grafische Oberflächen genutzt werden. Textbasierte und
im eingeschränkten Umfang auch grafische Verfahren bieten
die Möglichkeit, roboterspezifische
Datentypen zu deklarieren, einfache Bewegungen zu spezifizieren und
mit Werkzeugen und Sensoren zu interagieren. Zur Ausführung von
erstelltem Roboterprogrammcode wird dieser an die Robotersteuerung übertragen
und unter Einhaltung von Echtzeitbeschränkungen
ausgeführt.\vglcite[1]{muehe2010}

Automatische Systeme der Programmierung werden hingegen in
lernende Systeme, instruktive Systeme sowie demonstrative Systeme
unterteilt. Ein typisches Verfahren demonstrativer Programmierung ist das
Führen des Roboters über ein Teach Pendant oder durch die direkte
Führung der Roboterhand durch den Arbeitsbereich, wobei die dabei
entstehenden Gelenkwinkel oder
kartesische Tool Center Point Positionen (TCP-Positionen)
aufgezeichnet und im Programm hinterlegt werden. Lernende und instruktive System
nutzen Methoden des maschinellen Lernens (z.\,B. Reinforcement Learning) sowie
computerbasiertem Sehen (engl. Computer-Vision) zum Verständnis der Intention
und der Umwelt, oft in Rahmen der Mensch-Maschine Kollaboration
(MRK).\vglcite[4\psqq]{biggs2003}

Ergänzend zur methodischen Einordnung
wird bei der Roboterprogrammierung zwischen zwei
Durchführungsmodi unterschieden:
Online-Programmierung am realen System (z.\,B. Teachen per Teach
Pendant oder Handführung), bei der Wegpunkte und Ablauflogik
unmittelbar auf der Steuerung aufgezeichnet werden, und
Offline-Programmierung (OLP), bei der Programme in
virtuellen Zellen entworfen, geprüft und erst anschließend auf die
reale Steuerung übertragen
werden.\vglcites{\citesingle[1]{biggs2003}; \citesingle[62\psqq]{holubke2014}}

Online-Verfahren sind im industriellen Alltag verbreitet, verursachen
jedoch Stillstandszeiten und bergen
Test-Risiken.\vglcite[4]{bilancia2023} Hierbei können Beschädigungen am
Roboter, Maschinen, Werkstücken und Umwelt durch Kollisionen oder fehlerhafter
Konfiguration des Roboters auftreten. OLP verlagert
Entwurf, Kollisionsprüfung und Taktzeitabschätzung in die Simulation
und reduziert so Risiken und Anlagenstillstand.\vglcite[62\psqq]{holubke2014}
Somit wird die Entwicklung und Evaluierung
von Roboterprogrammen ohne physischen Roboter
ermöglicht: Programme werden in einer virtuellen Umgebung
erstellt, getestet und
iterativ verfeinert. Hierfür werden originalgetreue dreidimensionale
Computer Aided Design Modelle (3D-CAD-Modelle) des Roboters
und seiner Umgebung benötigt, welche teilweise vom Hersteller
bereitgestellt werden oder nachmodelliert werden müssen, um den
Zielkontext möglichst realitätsnah abzubilden. Ziel ist es durch die OLP
problematische Stellen des Roboterprogramms, Ablaufprobleme sowie
Nebenwirkungen des direkt und indirekt beeinflussten
Prozesses früh zu erkennen.
\vglcite[62\psqq]{holubke2014} Externe
Faktoren wie verformbare Objekte, Fluide oder Personen bei der MRK erhöhen im
Rahmen solcher Simulationen maßgeblich die
Komplexität. In realitätsnahen Szenarien (z.\,B. automatisierte
Steckverbindungen in Schaltschränken oder Gießen von Schmelzen) treten
materialbedingte Nichtidealitäten auf, die zu Wechselwirkungen mit dem Roboter
führen. Eine hinreichend genaue physikalische Modellierung des Zielsystems ist
daher Voraussetzung.

\subsection{Landschaft gängiger Programmierumgebungen}
Im Gegensatz zur
Werkzeugmaschinenprogrammierung, die auf standardisierten Sprachen wie G-Code
basiert, erschwert das Fehlen einer universellen, herstellerunabhängigen
Programmiersprache die Integration verschiedener Robotertechnologien in einer
Produktionsanlage.\vglcite[4]{bilancia2023} Industrielle
Roboterhersteller, beispielsweise KUKA, ABB,
Fanuc oder Stäubli, bieten und unterstützen lediglich eigene,
proprietäre Programmiersprachen
und Programmierschnittstellen, wobei diese sich in
Komplexität, Syntax und Semantik
unterscheiden.\vglcites{\citesingle[116]{lambrecht2011};
\citesingle[4]{bilancia2023}} Zudem müssen Roboterprogrammierer mit
eingeschränkten Basisbefehlen
und Bibliotheken arbeiten. Diese decken zwar die meisten Standardanforderungen
ab, ermöglichen jedoch keine fortgeschrittenen Berechnungen oder komplexen
Steuerungsstrategien. Offline-Programmierwerkzeuge wie RoboDK oder Siemens
Process Simulate übersetzen 3D-Modellierungsbefehle mithilfe spezifischer
Postprozessoren in herstellerspezifische Robotercodes. Allerdings unterstützen
diese Werkzeuge nicht die vollständigen Funktionsbibliotheken der kommerziellen
Robotersprachen und können erfahrene Programmierer bei komplexen
Programmierroutinen nicht ersetzen.\vglcite[4]{bilancia2023} Der
Versuch, eine einheitliche, standardisierte Programmiersprache für
alle Industrieroboter zu definieren und zu
verbreiten, scheitert dabei an der mangelnden Kooperation der
Hersteller industrieller Roboter.\vglcite[116]{lambrecht2011}

Einen gegensätzlichen Ansatz verfolgt das open-source Projekt
Robot Operating System (ROS). ROS ist ein quelloffenes
Middleware-Framework, das Bibliotheken und Werkzeuge für
Nachrichtenübertragung, Paketverwaltung und Hardwareabstraktion
bereitstellt und damit eine hersteller- und
plattformunabhängige Integrationsschicht
anstrebt.\vglcite[1]{quigley2009ros} In der industriellen Praxis wird
ROS/ROS~2 primär als Integrations-/Orchestrierungsebene
eingesetzt (z.\,B. Wahrnehmung, Planung, Zellenkoordination).
Echtzeitkritische Bewegungs- und Sicherheitsfunktionen verbleiben
üblicherweise auf herstellerseitig eingesetzten Robotersteuerungen, und der
dokumentierte industrielle Einsatz bleibt anwendungs- und
treiberabhängig.\vglcite[3-6]{bonci2023ros2} Da ROS~1
keine harten Echtzeitanforderungen der Robotersteuerung erfüllen
kann, soll ROS~2 dieses Defizit erfüllen.
Das erfordert in der Praxis jedoch ein echtzeitfähiges
Betriebssystem sowie sorgfältiges Systemtuning,
sodass ein reproduzierbares, deterministisches Ausführungsverhalten
weiterhin von Konfiguration und
Implementierung
abhängt.\vglcites{\citesingle[1]{maruyama2016ros2};
\citesingle[3-5]{bonci2023ros2}}

\subsection{Physik-Engines zur Simulation von Robotern}
Aufgrund der herstellerabhängig begrenzten Möglichkeiten der
realistischen Abbildung eines Szenarios bei der OLP, bietet es sich
an, auf Programme zur physikalischen Simulation von Körpern und Prozessen
zurückzugreifen. Mit dem Einsatz von
Physik-Engines soll eine virtuelle Umgebung geschaffen werden, in
dem die Realität nahezu realgetreu digital abgebildet werden kann
und so eine Simulation des Roboters und seiner Umgebung
geschaffen werden. Sie modellieren dynamische Interaktionen wie Kollisionen,
Schwerkraft und Reibung, was für die präzise Nachbildung des Roboterverhaltens
entscheidend ist. Obwohl die Genauigkeit dieser Engines als nicht perfekt
angesehen wird, da sie die reale Welt nicht exakt
abbilden\vglcite[1\psqq]{audonnet2022}, sind sie für
Forschung und Entwicklung essenziell, um physikalisch anspruchsvolle Prozesse
zuverlässig simulieren zu können. Die Wahl der Physik-Engine
beeinflusst die Stabilität und Wiederholbarkeit der
Simulation. Häufig genutzte Physik-Engines sind PhysX, Bullet und
ODE. In Tabelle \ref{table:simuplattform} werden gängige
Simulationsprogramme im Hinblick auf die verwendeten Physik-Engines,
ROS-Kompatibilität und deren Möglichkeit der Integration verglichen.
In der Praxis besteht ein Zielkonflikt zwischen numerischer Stabilität,
Reproduzierbarkeit (Determinismus) und physikalischer Genauigkeit. Die Wahl der
Engine beeinflusst daher nicht nur die Plausibilität der Dynamik, sondern auch
die Vergleichbarkeit von Simulationsergebnissen.\vglcite[1\psqq]{audonnet2022}

\begin{table}
  \begin{tabularx}{\columnwidth}{X|X|X|X|X} \toprule
    \thead{\textbf{Name}}        & \thead{\textbf{Physik- \newline Engine}} &
    \thead{\textbf{Open Source}} & \thead{\textbf{ROS-Integration}}         &
    \thead{\textbf{ML-Support}}
    \\ \midrule Gazebo                & Bullet, DART,
    ODE, Simbody                 & Ja                                       & Ja
    & Extern                                                \\ \hline Ignition
    & DART                                     & Ja
    & Ja                                       & Extern
    \\ \hline Webots                & ODE                                      &
    Ja                           & Ja
    & Extern
    \\ \hline Isaac Sim             & PhysX                                    &
    Nein                         & Ja
    & Integriert
    \\ \hline Unity                 & Havok, PhysX, RaiSim                     &
    Nein                         & Nein
    & Extern
    \\ \hline PyBullet              & Bullet                                   &
    Ja                           & Nein
    & Extern
    \\ \hline CoppeliaSim (V-rep)   & Newton, Bullet, ODE,Vortex Dynamics      &
    Nein                         & Ja
    & Extern
    \\ \hline Mujoco                & Mujoco                                   &
    Ja                           & Nein
    & Extern
    \\ \bottomrule
  \end{tabularx} \caption{Vergleich verschiedener
  Robotik-Simulationsplattformen, nach Bilancia}
  \label{table:simuplattform}
\end{table}

\section{Large Language Models} \label{sec:Grundlagen_LLMs}
Aufgrund kürzlicher Entwicklungen im Bereich der LLMs bieten diese durch ihre
Fähigkeit der Generierung textueller Inhalte eine vielversprechende Möglichkeit,
bei der Programmierung von Robotern zu unterstützen.

\subsection{Funktionsweise und Architektur}
Bei großen Sprachmodellen (LLMs) handelt es sich um statistische Modelle,
welche in der Lage sind textuelle Inhalte zu übersetzen, zusammenzufassen,
Informationen abzurufen und Konversation zu betreiben. Historisch sind LLMs aus
der Möglichkeit, neuronale Netze im Modus des self-supervised learning (deutsch:
selbst überwachtes Lernen) und anhand großer Mengen textueller
Trainingsdaten zu trainieren, entstanden.
Dabei haben sich LLMs innerhalb der letzten 10 Jahre aufgrund der
hohen Verfügbarkeit digitaler
textueller Daten sowie Innovationen im Bereich der Hardware-Technologie zur
wichtigsten Technologie im Bereich Künstliche Intelligenz (KI)
entwickelt. \vglcite[1\psq]{naveed2024}\\

  Den entscheidenden Durchbruch für moderne LLMs lieferte die
Transformer-Architektur, die sequenzielle Abhängigkeiten durch parallele
Attention-Mechanismen ersetzt.\vglcite[1\psqq]{vaswani2023attentionneed}
Mithilfe des
Self-Attention-Mechanismus wird für jedes Token die Relevanz zu allen
anderen Tokens einer Sequenz berechnet, wodurch ein Modell
kontextuelle Beziehungen direkt
erfasst, ohne Informationen sequenziell durch versteckte Schichten propagieren
zu müssen. Diese Parallelisierung ermöglicht nicht nur schnelleres Training auf
den erwähnten großen Datensätzen, sondern schafft auch die Grundlage für das
Skalierungsverhalten moderner Sprachmodelle. Folglich basieren aktuelle LLMs wie
GPT-4 oder Claude auf dieser Architektur.

\subsection{Programmcodegenerierung durch Large Language Models}%
LLMs haben bemerkenswerte Fortschritte in der automatischen
Code-Generierung erzielt und revolutionieren damit die Softwareentwicklung.
Code-LLMs sind in der Lage erfolgreich Quellcode aus natürlichsprachlichen
Beschreibungen zu generieren. Aktuelle LLMs demonstrieren anhand von
empirischen Benchmarks zur Evaluation der Qualität von
Programmiercode (z.\,B. HumanEval, MBPP und BigCodeBench), dass
sie in der Lage sind progressiv bessere Leistungen bei verschiedenen
Schwierigkeitsgraden und Programmieraufgaben zu erzielen. \vglcite[1]{jiang2024}
Voraussetzungen für die erfolgreiche Programmierung mithilfe von LLMs sind die
Miteinbeziehung von Programmiersprachen und dessen technische und syntaktische
Dokumentation in die Trainingsdaten des
jeweiligen Modells.

Domänenspezifische Code-Generierung stellt LLMs vor besondere
Herausforderungen, die deren praktische Anwendbarkeit erheblich einschränken.
Neue und weniger populäre Programmiersprachen (engl.: low-ressource
languages) sowie
domänenspezifische Programmiersprachen sind in
offen abrufbaren Datensätzen oft unterrepräsentiert, was zu einem Mangel
verfügbarer Trainingsdaten und erhöhten
Hürden durch spezialisierte Syntax führt.\vglcite[1]{joel2024} LLMs zeigen
zudem schwächere Leistungen beim Umgang mit domänenspezifischen Bibliotheken
\vglcite[1]{gu2025} – zusätzlicher Kontext (z.\,B. Repository-Code,
Schnittstellenbeschreibungen) ist hier erforderlich. Diese Problematik betrifft
Millionen von Entwicklern: Beispielsweise 3,5 Millionen Rust-Nutzer, einer
performanten und vergleichsweise neuartigen Programmiersprache,
können LLM-Funktionen nicht vollständig ausschöpfen.

Agentische KI-Systeme entwickeln sich zu einer neuen Generation
autonomer Softwareagenten, die komplexe Aufgaben selbstständig ohne
kontinuierliche
menschliche Anleitung ausführen können. Diese Systeme adressieren teilweise die
zuvor beschriebenen Herausforderungen bei domänenspezifischer Code-Generierung,
indem sie über Command-Line-Interfaces und externe Tools Zugang zu
spezialisierten Bibliotheken und Applicatio Programming Interfaces
(APIs) erhalten. Das Model Context Protocol (MCP)
etabliert sich als offener Standard zur Verbindung von AI-Assistenten mit
Datensystemen, Business-Tools und Entwicklungsumgebungen.\vglcite{anthropic2024}
Viele Softwareentwicklungstools bieten bereits MCP-Unterstützung, um KI-Agenten
besseren Zugang zu domänenspezifischen Kontextinformationen und
Code-Repositories zu ermöglichen. Die Standardisierung solcher Protokolle kann
eine Lösung für die Datenmangel-Problematik bei low-ressource languages
und domänenspezifischen Programmiersprachen darstellen, wobei die praktische
Wirksamkeit noch evaluiert werden muss. Für Robotik-Anwendungen ist dabei
entscheidend, dass agentische Systeme nicht nur natürlichsprachliche
Spezifikationen interpretieren, sondern deterministische, zeitkritische
Ausführungspfade erzeugen und an bestehende Steuerungsstacks andocken können.

\section{LLMs in der Robotik}

Die Integration von LLMs in die Robotik verfolgt drei
komplementäre Paradigmen: Vision-Language-Action Modelle, Code-Generation und
Embodied Reasoning.\vglcite[2\psqq]{salimpour2025}

Die zentralen Linien lassen sich knapp gliedern:
\begin{itemize}
  \item \textbf{Vision-Language-Action (VLA)}: Gemeinsame
    Repräsentation von Wahrnehmung, Sprache und Aktion.
  \item \textbf{Code-Generierung}: Synthese ausführbarer
    Kontrollprogramme aus natürlichsprachlichen Spezifikationen.
  \item \textbf{Embodied Reasoning}: Multimodale Modelle mit
    kontinuierlichen Sensordaten und Weltmodellen.
\end{itemize}

\subsection{Aktuelle Forschungsansätze}
Google DeepMinds RT-2 repräsentiert den Vision-Language-Action Ansatz, bei dem
Roboter-Aktionen als Text-Token behandelt und gemeinsam mit visuellen und
sprachlichen Daten trainiert werden. Dieser Ansatz ermöglicht emergente
Fähigkeiten wie das Verstehen von Zahlen oder Icons ohne explizites
Training.\vglcite[1\psqq]{brohan2023} Code as Policies verfolgt hingegen die
direkte Generierung von ausführbarem Python-Code aus natürlichsprachlichen
Befehlen, wobei durch hierarchische Code-Generation komplexe
Kontrollstrukturen wie
Schleifen und Bedingungen erzeugt werden.\vglcite[1\psqq]{liang2023}
Das Modell PaLM-E demonstriert einen dritten Weg durch multimodale
Embodied Language Models, die
Sprache, Vision und kontinuierliche Sensordaten in einem gemeinsamen
Embedding-Raum verarbeiten\vglcite[2\psq]{driess2023}. Parallel entwickeln
Forscher Brain-Body-Problem Ansätze, die kognitive Architekturen mit physischen
Roboterkörpern verbinden sowie Prompt-basierte Methoden, bei denen LLMs direkt
Low-Level-Kontrollaktionen vorhersagen.\vglcites{\cite[1]{wang2024};
\cite[2]{bhat2024}} Diese Vielfalt der Ansätze zeigt, dass die Forschung noch
keine dominante Architektur etabliert hat und
verschiedene Wege zur Integration von Sprache und Robotik
exploriert.\vglcite[3\psqq]{salimpour2025}

\subsection{Herausforderungen in der Integration}

Die Verankerung abstrakter Sprachkonzepte in physischen Robotersystemen
scheitert an drei fundamentalen Problemen. Erstens müssen Roboter symbolische
Repräsentationen mit sensomotorischen Erfahrungen verknüpfen - das von Harnad
definierte Symbol-Grounding-Problem bleibt trotz jahrzehntelanger Forschung
ungelöst \vglcite[1\psqq]{cohen2024}. Zweitens fehlen standardisierte
Schnittstellen zwischen hochabstrakten Sprachbefehlen und niedere, hardwarenahe
Motorkommandos, wodurch jede Roboterplattform individuelle
Übersetzungsmechanismen benötigt. Drittens limitieren Echtzeitanforderungen die
Komplexität der Verarbeitung, da Roboter innerhalb von Millisekunden auf
Umweltveränderungen reagieren müssen. Moderne Systeme versuchen diese
Herausforderungen durch die Integration dreier Wahrnehmungsebenen zu lösen:
Interozeption erfasst interne Zustände, Propriozeption überwacht
Gelenkstellungen und Bewegungen, während Exterozeption die Umgebung durch
Kameras und Sensoren interpretiert \vglcite[3,13]{valenzo2022}. Jedoch führt
diese Komplexität zu erhöhtem Rechenaufwand und erschwert die Fehlerdiagnose bei
unerwarteten Verhaltensweisen. Folglich benötigen robotische Systeme neue
Architekturen, die effizient zwischen abstrakten Sprachmodellen und konkreten
Aktionsräumen vermitteln. In der Literatur werden deshalb hybride Architekturen
diskutiert, die symbolische Planung, differenzierbare Wahrnehmung und reaktive,
zeitkritische Kontrolle kombinieren. Für industrielle Szenarien bleibt die Frage
zentral, wie viel Autonomie ein LLM-gestütztes System erhalten kann, ohne
Validierbarkeit und Betriebssicherheit zu kompromittieren.

\subsection{Bestehende Frameworks und Tools}

Aktuelle Frameworks standardisieren die Integration von LLMs in robotische
Systeme durch modulare Architekturen. ROS-LLM verbindet das Robot Operating
System mit verschiedenen Sprachmodellen und transformiert natürlichsprachliche
Befehle automatisch in ausführbare Aktionssequenzen \vglcite[1\psq]{mower2024}.
Das Framework implementiert drei Ausführungsmodi: sequenzielle Abarbeitung für
einfache Aufgaben, Verhaltensbäume für reaktive Systeme und Zustandsautomaten
für komplexe Ablaufsteuerungen. Entwickler konfigurieren atomare Aktionen wie
das Greifen eines Objektes oder bestimmte Pfadnavigationen, die das LLM dann zu
komplexen Verhaltensketten kombiniert. Simulationsumgebungen beschleunigen
parallel die Entwicklung durch massiv-parallele GPU-Berechnungen – mit Isaac Sim
von NVIDIA lassen sich beispielsweise Tausende Roboterinstanzen gleichzeitig
simulieren.\vglcite{NVIDIATechBlog2018} Solche Plattformen
ermöglichen einen theoretischen
Zero-Shot-Transfer von der Simulation zur Realität durch systematische
simulierte Domänenrandomisierung von Physikparametern, Sensorrauschen und
Umgebungsvariationen.\vglcite[1]{fickinger2025} Die Standardisierung solcher
Werkzeugketten soll Entwicklungszeiten in Zukunft reduzieren und die
Programmierung von
Robotern niederschwelliger machen.

\section{Zwischenfazit und Forschungsfrage}

Die bisherigen Grundlagen zeigen: Simulation und
Offline-Programmierung sind zentrale Bausteine, um Robotikprozesse
vorab sicher und nachvollziehbar zu prüfen. LLMs eröffnen dabei die
Möglichkeit, Roboterprogrammcode aus natürlicher Sprache zu generieren,
stoßen in industriellen Kontexten jedoch auf die Anforderung,
Ergebnisse systematisch und reproduzierbar zu validieren.

Daraus leitet sich die zentrale Frage, wie gut sich
von LLMs erzeugter Robotercode in einer simulationsbasierten
Umgebung hinsichtlich prozessrelevanter Aspekte
prüfen und bewerten lässt.

Zur Beantwortung wird in den folgenden Kapiteln eine geeignete
Umgebung zur Simulation eines Roboters aufgebaut und an einem
kompakten, realitätsnahen
Szenario erprobt. Ziel ist das Ausführen von Robotercode innerhalb dieser
Umgebung und belastbare Prüfung nach definierten möglichen Fehler im
Programmablauf mit nachvollziehbarer Dokumentation der Ergebnisse.
Dazu beschreibt Kapitel~\ref{sec:framework} dazu die Architektur der Frameworks,
die verschiedenen zu prüfenden Fehlerarten sowie das
Testszenario. Weiterführend wird durch Anwendung des Frameworks in
Kapitel~\ref{cap:Ergebnisse} ausgewertet,
inwiefern Fehler im Roboterprogrammcode
erkannt werden sowie welche
Struktur und Inhalt dessen Dokumentation folgen.
